{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = models.resnet18()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "    valdir = os.path.join(args.data, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "    if args.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion)\n",
    "        return\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch    model = models.resnet18()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "            top5.update(prec5[0], input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                       i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                       top1=top1, top5=top5))\n",
    "\n",
    "        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "'''Train CIFAR10 with PyTorch.'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import tensorboard_logger as logger\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "\n",
    "import calendar\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N')\n",
    "parser.add_argument('--test-batch-size', type=int, default=128, metavar='N')\n",
    "parser.add_argument('--epochs', type=int, default=340, metavar='N')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S')\n",
    "parser.add_argument('--log-interval', type=int, default=25, metavar='N')\n",
    "parser.add_argument('--save-interval', type=int, default=50, metavar='N')\n",
    "parser.add_argument('--validate-interval', type=int, default=1, metavar='N')\n",
    "parser.add_argument('--optimizer', type=str, default='momentum_sgd')\n",
    "parser.add_argument('--model', type=str, required=True)\n",
    "parser.add_argument('--model-import', type=str, required=True)\n",
    "parser.add_argument('--lr-decay', type=float, default=0.1)\n",
    "parser.add_argument('--lr-decay-rate', type=float, default=100)\n",
    "parser.add_argument('--weight-decay', type=float, default=1e-4)\n",
    "parser.add_argument('--no-tensorboard', action='store_true', default=False)\n",
    "parser.add_argument('--devices', type=int, default=2, metavar='N')\n",
    "parser.add_argument('--cifar', type=int, default=10, metavar='N')\n",
    "parser.add_argument('--l', type=int, default=100, metavar='N')\n",
    "parser.add_argument('--growth-rate', type=int, default=12, metavar='N')\n",
    "parser.add_argument('--cs_planes', type=int, default=10, metavar='N')\n",
    "parser.add_argument('--top', type=float, default=0.0, metavar='N')\n",
    "parser.add_argument('--print-model-only', action='store_true', default=False)\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "model = __import__(args.model_import)   \n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('Preparing data...')\n",
    "\n",
    "if args.cifar==100:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Normalize((0.50707516, 0.48654887, 0.44091784), (0.26733429, 0.25643846, 0.27615047)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Normalize((0.50707516, 0.48654887, 0.44091784), (0.26733429, 0.25643846, 0.27615047)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "else: \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "\n",
    "# Model\n",
    "if args.resume:\n",
    "    # Load checkpoint.\n",
    "    print('Resuming from checkpoint...')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/' + 'model_{}'.format(args.model))\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "else:\n",
    "    print('Building model...')\n",
    "    if \"memnet\" in args.model_import:\n",
    "        net = model.MemNet(args.cifar, \n",
    "            l=args.l, \n",
    "            growth_rate=args.growth_rate, \n",
    "            cs_planes=args.cs_planes, \n",
    "            top=args.top)\n",
    "    else:\n",
    "        net = model.MemNet(args.cifar)\n",
    "\n",
    "    if args.print_model_only:\n",
    "        model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        print(\"#Params: \" + str(params))\n",
    "        print(net)        \n",
    "        sys.exit()\n",
    "\n",
    "if USE_CUDA:\n",
    "    net.cuda()\n",
    "\n",
    "    devices = []\n",
    "    for i in range(args.devices):\n",
    "        devices.append(i)\n",
    "\n",
    "    net = torch.nn.DataParallel(net, device_ids=devices)\n",
    "\n",
    "    # Enable benchmark mode, where CUDA finds best algorithm for h/w\n",
    "    # Works well if input size doesn't change much\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[args.epochs*.25, args.epochs*.5,args.epochs*.75], gamma=0.1)\n",
    "\n",
    "def adjust_lr(new_lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    # # After the first 10 epochs, set LR back to 0.1\n",
    "    # if epoch==10:\n",
    "    #     adjust_lr(0.1, optimizer)\n",
    "    #     print(\"Set to new learning rate \" + str(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "\n",
    "    # Step count for decaying learning rate\n",
    "    scheduler.step()\n",
    "    if not args.no_tensorboard:\n",
    "        tb_logger.add_scalar('misc/learning_rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    epoch_start_time = calendar.timegm(time.gmtime())\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if USE_CUDA:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_mini_loss = loss.data[0]\n",
    "        curr_mini_size = targets.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        curr_mini_correct = predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        train_loss += curr_mini_loss\n",
    "        total += curr_mini_size\n",
    "        correct += curr_mini_correct\n",
    "\n",
    "        if batch_idx%args.log_interval == 0:\n",
    "            train_loss_avg = train_loss/(batch_idx+1)\n",
    "            train_acc_avg = 100.*correct/total\n",
    "\n",
    "            msg = 'Train Loss Avg: %.3f | Train Acc Avg: %.3f%% (%d/%d)' % (train_loss_avg, train_acc_avg, correct, total)\n",
    "            print(msg)\n",
    "            if not args.no_tensorboard:\n",
    "                tb_logger.add_text('train', msg, epoch)\n",
    "\n",
    "            if not args.no_tensorboard:\n",
    "                tb_logger.add_scalar('train/train_loss', train_loss_avg, epoch)\n",
    "                tb_logger.add_scalar('train/train_accuracy', train_acc_avg, epoch)\n",
    "                tb_logger.add_scalar('train/train_error', 100-train_acc_avg, epoch)\n",
    "\n",
    "    epoch_end_time = calendar.timegm(time.gmtime())\n",
    "    epoch_time_taken = epoch_end_time - epoch_start_time\n",
    "\n",
    "    print(\"epoch_time_taken: \" + str(epoch_time_taken))\n",
    "\n",
    "    if not args.no_tensorboard:\n",
    "        tb_logger.add_scalar('train/epoch_time_taken', epoch_time_taken, epoch)\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    test_start_time = calendar.timegm(time.gmtime())\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        if USE_CUDA:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets, volatile=True)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        curr_mini_loss = loss.data[0]\n",
    "        curr_mini_size = targets.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        curr_mini_correct = predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        test_loss += curr_mini_loss\n",
    "        total += curr_mini_size\n",
    "        correct += curr_mini_correct\n",
    "\n",
    "    test_end_time = calendar.timegm(time.gmtime())\n",
    "    test_time_taken = test_end_time - test_start_time\n",
    "\n",
    "    print(\"test_time_taken: \" + str(test_time_taken))\n",
    "\n",
    "    test_loss_avg =  test_loss/(batch_idx+1)\n",
    "    test_acc_avg = 100.*correct/total\n",
    "\n",
    "    msg = 'Test Loss Avg: %.3f | Test Acc Avg: %.3f%% (%d/%d)' % (test_loss_avg, test_acc_avg, correct, total)\n",
    "    print(msg)\n",
    "    if not args.no_tensorboard:\n",
    "        tb_logger.add_text('test', msg, epoch)\n",
    "\n",
    "    if not args.no_tensorboard:\n",
    "        tb_logger.add_scalar('test/test_loss', test_loss_avg, epoch)\n",
    "        tb_logger.add_scalar('test/test_accuracy', test_acc_avg, epoch)\n",
    "        tb_logger.add_scalar('test/test_error', 100-test_acc_avg, epoch)\n",
    "        tb_logger.add_scalar('test/time_taken', test_time_taken, epoch)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if (epoch+1)%args.save_interval == 0:\n",
    "        print('Saving...')\n",
    "        tb_logger.add_text('misc', \"Saving\", epoch)\n",
    "\n",
    "        state = {\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'net': net.module if USE_CUDA else net,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/' + args.model_import+'_{}'.format(args.model))\n",
    "        if acc>best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "tb_logger = None\n",
    "if not args.no_tensorboard:\n",
    "    if \"memnet\" in args.model_import:\n",
    "        log_dir = os.path.join('log',\n",
    "                    '{}'.format(args.model_import), \n",
    "                     '{}'.format(args.model),\n",
    "                     'cifar_{}'.format(args.cifar), \n",
    "                     'lr_{}'.format(args.lr),\n",
    "                     'L_{}'.format(args.l),\n",
    "                     'GR_{}'.format(args.growth_rate), \n",
    "                     'CS_{}'.format(args.cs_planes),\n",
    "                     'TOP_{}'.format(args.top), \n",
    "                     datetime.now().isoformat())\n",
    "    else:\n",
    "        log_dir = os.path.join('log',\n",
    "            '{}'.format(args.model_import), \n",
    "             '{}'.format(args.model), \n",
    "             'cifar_{}'.format(args.cifar),\n",
    "             'lr_{}'.format(args.lr), \n",
    "             datetime.now().isoformat())\n",
    "\n",
    "    tb_logger = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+args.epochs):\n",
    "    train(epoch)\n",
    "    if epoch==0 or (epoch+1)%args.validate_interval==0:\n",
    "        test(epoch)\n",
    "\n",
    "tb_logger.close()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlinputs import tarrecords\n",
    "from dlinputs import gopen\n",
    "from dlinputs import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = gopen.sharditerator(\"test/testdata/cifar10-train-@000001.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = gopen.sharditerator(\"gs://lpr-demo/cifar10-train-@000001.tgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = gopen.sharditerator(\"test/testdata/imagenet-@000001.tgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__key__': '10',\n",
       " '__source__': None,\n",
       " 'cls': 304,\n",
       " 'png': array([[[0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       " \n",
       "        [[0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       " \n",
       "        [[0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       " \n",
       "        [[0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]],\n",
       " \n",
       "        [[0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         ...,\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686],\n",
       "         [0.99215686, 0.99215686, 0.99215686]]], dtype=float32),\n",
       " 'wnid': b'n04380533',\n",
       " 'xml': b'None'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
